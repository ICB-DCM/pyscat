"""Helper for objective evaluation during scatter search."""

from __future__ import annotations

import multiprocessing
import threading
from collections.abc import Sequence
from concurrent.futures import ThreadPoolExecutor
from copy import deepcopy

import numpy as np
from pypesto import Problem
from pypesto.startpoint import StartpointMethod

__all__ = [
    "FunctionEvaluator",
    "FunctionEvaluatorMT",
    "FunctionEvaluatorMP",
    "create_function_evaluator",
]


class TooManyFailuresError(RuntimeError):
    """
    Raised when too many failures to obtain a finite objective value occur.
    """


class FunctionEvaluator:
    """Wrapper for optimization problem and startpoint method.

    Takes care of (potentially parallel) function evaluations, startpoint
    sampling, and tracks number of function evaluations.

    :ivar problem: The problem
    :ivar startpoint_method: Method for choosing feasible parameters
    :ivar n_eval: Number of objective evaluations since construction
        or last call to ``reset_counter``.
    """

    def __init__(
        self,
        problem: Problem,
    ):
        """Construct.

        :param problem: The problem
        """
        self.problem: Problem = problem
        self.startpoint_method: StartpointMethod = problem.startpoint_method
        self.n_eval: int = 0
        # Maximum number of consecutive failures to obtain a finite objective
        #  value when sampling random points.
        #  This is to prevent infinite loops.
        self.max_failures: int = 1000

    def single(self, x: np.ndarray) -> float:
        """Evaluate objective at point ``x``.

        :return: The objective function value at ``x``.
        """
        self.n_eval += 1
        return self.problem.objective(x)

    def multiple(self, xs: Sequence[np.ndarray] | np.ndarray) -> np.ndarray:
        """Evaluate objective at several points.

        :param xs: Sequence of parameter vectors at which the objective is to
            be evaluated.

        :return: The objective function values in the same order as the inputs.
        """
        res = np.fromiter(map(self.single, xs), dtype=float)
        return res

    def single_random(self) -> tuple[np.ndarray, float]:
        """Evaluate objective at a random point.

        The point is generated by the given ``startpoint_method``. A new point
        will be generated until a finite objective value is obtained.

        :return: Tuple of the generated parameter vector and the respective
            function value.
        """
        for _ in range(self.max_failures):
            x = self.startpoint_method(n_starts=1, problem=self.problem)[0]
            fx = self.single(x)

            if np.isfinite(fx):
                break
        else:
            raise TooManyFailuresError(
                f"Failed to obtain a finite objective value after "
                f"{self.max_failures} attempts."
            )

        return x, fx

    def multiple_random(self, n: int) -> tuple[np.ndarray, np.ndarray]:
        """Evaluate objective at ``n`` random points.

        The points are generated by the given ``startpoint_method``. New points
        will be generated until only finite objective values are obtained.

        :param n: Number of random points to generate and evaluate.
        :return: Tuple of the generated parameter vectors and the respective
            function values.
        """
        fxs = np.full(shape=n, fill_value=np.nan)
        xs = np.full(shape=(n, self.problem.dim), fill_value=np.nan)
        n_failures = -1
        while not np.isfinite(fxs).all():
            retry_indices = np.argwhere(~np.isfinite(fxs)).flatten()
            xs[retry_indices] = self.startpoint_method(
                n_starts=retry_indices.size, problem=self.problem
            )
            fxs[retry_indices] = self.multiple(xs[retry_indices])
            # we're not exactly counting consecutive objective evaluation
            # failures here, but this is good enough to prevent infinite loops.
            n_failures += 1
            if n_failures >= self.max_failures:
                raise TooManyFailuresError(
                    f"Failed to obtain finite objective values "
                    f"after {self.max_failures} attempts."
                )
        return xs, fxs

    def reset_counter(self):
        """Reset the overall function counter."""
        self.n_eval = 0

    def _initialize_worker(self, local: threading.local):
        """Initialize worker threads."""
        # create a copy of the objective to maybe be thread-safe.
        local.objective = deepcopy(self.problem.objective)


class FunctionEvaluatorMT(FunctionEvaluator):
    """FunctionEvaluator with thread-parallel evaluation."""

    def __init__(
        self,
        problem: Problem,
        n_threads: int,
    ):
        """Construct.

        :param problem: The problem
        :param n_threads: Maximum number of threads to use for parallel
            objective function evaluations.
            Requires the objective to be copy-able,
            and that copies are thread-safe.
        """
        super().__init__(problem=problem)

        # Number of threads for parallel objective evaluation
        self._n_threads: int = n_threads

        self._init_threading()

    def __getstate__(self):
        return {
            k: v
            for k, v in vars(self).items()
            if k not in {"_thread_local", "_executor"}
        }

    def __setstate__(self, state):
        vars(self).update(state)
        self._init_threading()

    def _init_threading(self):
        """Initialize multi-threading-related attributes."""
        # Thread-local storage for copies of objective. Each thread gets its
        #  own copy of the objective, which may be sufficient to make some
        #  objectives thread-safe.
        self._thread_local: threading.local = threading.local()
        # The thread-pool to be used for parallel objective evaluations
        self._executor: ThreadPoolExecutor | None = (
            ThreadPoolExecutor(
                max_workers=self._n_threads,
                thread_name_prefix=__name__,
                initializer=self._initialize_worker,
                initargs=(self._thread_local,),
            )
            if self._n_threads > 1
            else None
        )

    @staticmethod
    def _evaluate_on_worker(
        local_and_x: tuple[threading.local, np.ndarray],
    ) -> float:
        """Task handler on worker threads."""
        local, x = local_and_x
        return local.objective(x)

    def multiple(self, xs: Sequence[np.ndarray]) -> np.ndarray:
        """Evaluate objective at several points.

        :param xs: Sequence of parameter vectors at which the objective is to
            be evaluated.

        :return: The objective function values in the same order as the inputs.
        """
        if self._executor is not None:
            res = np.fromiter(
                self._executor.map(
                    self._evaluate_on_worker,
                    ((self._thread_local, x) for x in xs),
                ),
                dtype=float,
            )
            self.n_eval += len(xs)
        else:
            res = np.fromiter(map(self.single, xs), dtype=float)
        return res


class FunctionEvaluatorMP(FunctionEvaluator):
    """FunctionEvaluator with process-parallel evaluation."""

    def __init__(
        self,
        problem: Problem,
        n_procs: int,
    ):
        super().__init__(problem=problem)
        self._pool = multiprocessing.Pool(
            n_procs,
            # initializer=self._initialize_worker,
            # initargs=(self._thread_local,),
        )

    def multiple(self, xs: Sequence[np.ndarray]) -> np.ndarray:
        """Evaluate objective at several points.

        :param xs: Sequence of parameter vectors at which the objective is to
            be evaluated.

        :return: The objective function values in the same order as the inputs.
        """
        res = np.fromiter(
            self._pool.map(self.problem.objective, xs),
            dtype=float,
        )
        self.n_eval += len(xs)
        return res


def create_function_evaluator(
    problem: Problem = None,
    n_procs: int = None,
    n_threads: int = None,
):
    """Create a FunctionEvaluator.

    Based on multiprocessing or multithreading, depending on whether
    ``n_procs`` (number of processes) or ``n_threads`` (number of threads)
    is specified. If neither is specified, a single-threaded evaluator is
    returned.
    """
    if n_procs and n_threads:
        raise ValueError(
            "Only one of `n_procs` and `n_threads` may be specified."
        )

    if n_procs:
        return FunctionEvaluatorMP(
            problem=problem,
            n_procs=n_procs,
        )

    return FunctionEvaluatorMT(
        problem=problem,
        n_threads=n_threads or 1,
    )
